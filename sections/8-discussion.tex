\section{Discussion}

The preceding experiments reveal a consistent and reproducible pattern: content optimized using the GEO framework demonstrates substantial improvements in citation frequency within generative engine outputs---regardless of its SEO status, backlink presence, or domain authority. This section synthesizes the theoretical implications, operational considerations, and real-world constraints identified across both case studies.

\subsection{From Ranking to Referencing}

The SOYA course trial demonstrated that even when a page is already ranked highly in traditional search (e.g., first result on Google), it may remain completely invisible to generative engines like ChatGPT or Perplexity if it lacks semantic modularity and extractability. After applying structured Answer Layer rewriting, schema markup, and paragraph modularization, the same content was cited in over 77\% of generated segments---a finding that mirrors the citation uplift seen in GEO-Bench \cite{aggarwal2024geo}.

In contrast, the “Tianyou Liao” name-based trial involved a page that ranked poorly in SEO (GSC average position 10.7) and shared its target query with a dominant, unrelated entity. Even so, the GEO-structured content achieved a 60\%+ citation rate in both generative engines---suggesting that well-engineered semantic structure can overcome even \textbf{disambiguation bias}.

Together, these findings support the core GEO hypothesis: \textbf{LLM visibility is governed more by extractability, coherence, and topical clarity than by link-based authority signals}.

\subsection{Layer Effectiveness and Interaction}

The experiments further validate the functional independence and interaction of the three GEO semantic layers:

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Layer} & \textbf{Contribution Evidence} & \textbf{Observed Effects} \\\hline
Semantic Anchoring & Clear titles and summaries & Higher classification accuracy \\\hline
Context Triggering & Synonymic phrasing, taxonomy & Broader query matching \\\hline
Pragmatic Recomposition & FAQs, modular paragraphs & Mid-body citation precision \\\hline
\end{tabular}
\end{center}

Notably, ChatGPT cited not only the introductory paragraphs, but also embedded FAQ answers and sentence-level definitions---reinforcing the role of Layer 3 in enabling recomposition. This behavior is consistent with Lüttgenau et al.’s BART fine-tuning results \cite{luttgenau2025beyondseo}.

\subsection{Visibility \texorpdfstring{$\neq$}{≠} Authority}


One counterintuitive outcome is the citation of low-authority domains (e.g., GitHub Pages with no backlinks) over highly ranked official sites. This challenges a long-standing SEO assumption: that PageRank correlates with trust or visibility.

In GEO, trust emerges from \textbf{information packaging}, not \textbf{linkage popularity}. This suggests that smaller or emerging creators can compete on equal footing if they invest in semantic clarity and modular formatting.

\subsection{Constraints and Limitations}

\begin{itemize}
  \item \textbf{LLM output variability:} Generative results may vary slightly depending on prompt, model version, or context.
  \item \textbf{Crawling uncertainty:} It remains impossible to directly verify when an LLM ingested a newly published site.
  \item \textbf{Black-box citation logic:} LLMs may cite based on latent embeddings or internal heuristics not fully transparent to users.
\end{itemize}

These limitations are consistent with the “partially observable” nature of GEO described by Aggarwal et al. \cite{aggarwal2024geo}, and reinforce the need for multi-session testing and layered metric evaluation.

\subsection{Validation of GEO's Three-Layer Model}

Across all case studies and experiments conducted, our results provide strong empirical support for the functional effectiveness of GEO’s proposed three-layer semantic visibility framework. Specifically, we observe:

\begin{itemize}
  \item \textbf{Layer 1: Semantic Anchoring} was consistently validated across domains. Clear topical headings, descriptive summaries, and introductory framing led to robust inclusion in generative outputs, even when other structural formatting was absent (Sections 6.1, 7.4).
  
  \item \textbf{Layer 3: Pragmatic Recomposition} proved to be the most reliable citation trigger, particularly in FAQ-style queries. Modular paragraph design, list formatting, and structured Q\&A blocks significantly enhanced citation likelihood (Sections 6.1–6.3).
  
  \item \textbf{Layer 2: Context Triggering}, initially less observable in single-domain settings, was subsequently validated in Section 6.5 via a dedicated cross-domain experiment. We demonstrated that paraphrastic phrasing, synonymous terminology, and taxonomic generalizations substantially improved recall across semantically varied queries.
\end{itemize}

Together, these findings confirm that the three layers, while distinct in mechanism, operate synergistically to influence LLM citation behavior. Effective content design for generative engine visibility therefore requires simultaneous attention to anchoring clarity, semantic breadth, and modular composition.

\subsection{Interpretation of Layer 2 Findings}

The experimental validation of Layer 2 (Context Triggering) revealed both promising outcomes and inherent complexities. On the one hand, our cross-domain tests confirmed that semantically enriched content—containing paraphrases, taxonomic reformulations, and synonymous variants—consistently ranked among Top-5 citation results, even under significant surface variation in queries. This affirms the theoretical basis of Layer 2 as a semantic “bridge” that supports retrievability beyond anchor-term matching.

However, the effect size was domain-dependent. Domains with dense synonym networks (e.g., health, economics) saw greater citation stability than jargon-heavy or emerging domains with sparse paraphrastic diversity. Additionally, we observed that when content lacked clear anchoring (Layer 1) or modular formatting (Layer 3), Layer 2 enhancements alone were insufficient to guarantee citation—suggesting inter-layer synergy remains necessary.

These findings highlight that while Layer 2 functions as a standalone mechanism for semantic generalization, its optimal impact is realized when combined with strong anchoring and modular composition. Future work should explore adaptive scoring mechanisms to quantify layer-level interactions, and model sensitivity to various types of contextual perturbation.
