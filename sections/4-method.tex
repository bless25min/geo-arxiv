\section{Methodology: Technical Implementation of GEO}

To operationalize the three-layer semantic visibility model, we constructed a multi-stage GEO implementation pipeline using publicly available tools and lightweight deployment infrastructure. This section details the real-world execution process, emphasizing reproducibility, modularity, and platform neutrality.

\subsection{Content Generation and Structuring}

Initial content was drafted by dictating past experience and project descriptions using ChatGPT’s voice-to-text transcription. Supporting materials such as research notes and structured knowledge were integrated via ChatGPT’s linker and research tools.

Each content block was revised to:
\begin{itemize}
  \item Maintain modular paragraphing (3--5 sentences)
  \item Use informative, scoped H2/H3 headers
  \item Provide standalone blocks such as lists, FAQs, and short definitions
\end{itemize}

This directly supports Layer 1 and Layer 3 of the semantic visibility model.

\subsection{Deployment via Semantic Mesh}

Using Claude 3’s “Project Knowledge” export function, structured content was published as static HTML files. These were organized on GitHub Pages under a mesh architecture:
\begin{itemize}
  \item \textbf{Pillar nodes:} author profile, main topic overview
  \item \textbf{Cluster nodes:} grouped pages (e.g., SEO, LLM, case studies)
  \item \textbf{Mini nodes:} specific modules or tools
\end{itemize}

Each page interlinked upward toward its Pillar node, maintaining semantic cohesion and low crawl depth.

\subsection{Schema Integration}

Pages used Schema.org markup in JSON-LD format. Depending on content type, we added:
\begin{itemize}
  \item \texttt{Article} and \texttt{FAQPage} for explanatory and Q\&A blocks
  \item \texttt{Person} and \texttt{WebPage} for biography and overview pages
\end{itemize}

Markup was validated via Google’s Rich Results Test.

\subsection{Indexing and Monitoring}

Pages were submitted through Google Search Console. During the 7/6--7/15 observation period, indexation coverage was confirmed. Although average rank was low (position 10.7), citation occurred in generative responses before reaching top SERP.

\subsection{Summary of Toolchain}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Step} & \textbf{Tool} & \textbf{Output} \\\hline
Dictation & ChatGPT Voice & Modular paragraphs \\\hline
Knowledge Structuring & ChatGPT Linker & Topic clusters \\\hline
Export & Claude Project Knowledge & HTML pages \\\hline
Hosting & GitHub Pages & Semantic Mesh \\\hline
Schema Testing & Rich Results Test & JSON-LD validation \\\hline
Index Monitoring & Google Search Console & Coverage + rank data \\\hline
Evaluation & ChatGPT/Perplexity (incognito) & Citation rates \\\hline
\end{tabular}
\end{center}

\subsection{Cross-Layer Experimental Framework}

To validate the independent and combined effects of all three GEO semantic layers, we constructed a controlled simulation pipeline involving semantically rich queries, annotated paragraph corpora, and neural semantic retrieval.

\subsubsection{Query Set Design}
We selected 10 representative knowledge domains—such as artificial intelligence, health policy, macroeconomics, and education—and for each, authored five semantically equivalent query variants:
\begin{itemize}
  \item \textbf{Original (Baseline)}: Direct phrasing aligned with canonical domain terminology.
  \item \textbf{Synonym}: Reworded version using casual or less common substitutes for key terms.
  \item \textbf{Classification}: Query framed using broader taxonomic categories.
  \item \textbf{Rare-term}: Query using domain-specific jargon or rarely used synonyms.
  \item \textbf{FAQ-style}: Natural question phrasing matching conversational use.
\end{itemize}
All queries targeted the same underlying information need to isolate linguistic variation effects (Layer 2).

\subsubsection{Corpus Construction}
Each domain was paired with a manually curated corpus of 400–600 short paragraphs (total $>$5000), built to reflect varying combinations of Layer 1, 2, and 3 properties. Paragraphs were annotated based on:
\begin{itemize}
  \item \textbf{Anchoring (L1)}: Presence of clear titles, definitions, and summary structures.
  \item \textbf{Contextual Expansion (L2)}: Inclusion of synonyms, taxonomy terms, or paraphrases.
  \item \textbf{Modularity (L3)}: FAQ blocks, lists, numbered steps, and concise statements.
\end{itemize}
Paragraphs were either synthesized using research prompts and large model outputs or extracted from cleaned Wikipedia/educational sources with human editing.

\subsubsection{Retrieval Pipeline}
We embedded all queries and corpus paragraphs using Sentence-BERT (all-MiniLM-L6-v2). For each query, the model returned the Top-5 most semantically similar paragraphs (based on cosine similarity). These were treated as citation candidates, simulating LLM-style retrieval grounding.

\subsubsection{Annotation and Scoring}
Retrieved citations were then analyzed for:
\begin{itemize}
  \item Whether the paragraph had Layer 2 markings (as pre-tagged in corpus).
  \item How many citations overlapped between each query variant and the original (Top-$k$ overlap).
  \item The consistency of result rankings via Kendall’s $\tau$.
  \item Semantic embedding distance from query variants to original.
\end{itemize}
No LLM was queried directly; this setup isolates layer-level effects within a controlled semantic similarity system, without black-box model interference.
