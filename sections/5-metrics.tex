\section{Evaluation Metrics} \label{sec:metrics}

To systematically assess the effectiveness of GEO interventions, we define five complementary evaluation metrics that measure different dimensions of content visibility within generative engines. These indicators are grounded in existing citation theories \cite{aggarwal2024geo, liu2023verifiability, menick2022quotes} and adapted for real-world observability and automation.

\subsection{AIO Semantic Focus Score}

\textbf{Definition:} The share of a document’s sentences that exhibit high semantic alignment with a declared topic or entity of focus.

Let $S$ be the total number of sentences and $S_t \subset S$ be those that contain or reinforce the primary topic entity $T$, as annotated via Named Entity Recognition (NER) or manual labeling.

\[
\text{AIO\_Semantic\_Focus}(T) = \frac{|S_t|}{|S|}
\]

\textbf{Threshold:} $\geq 0.75$

\textbf{Usage:} Diagnoses clarity of anchoring (Layer 1). A low score implies excessive topic drift.

\subsection{Citation Potential Score}

\textbf{Definition:} A composite metric scoring a paragraph or document on four citation-enhancing dimensions identified in GEO-Bench: factual density, authoritativeness, utility, and presentation clarity.

We adapt a simplified linear aggregation based on normalized human rater scores or GPT-assisted rubric scoring (following G-Eval templates).

\[
\text{Citation\_Potential} = \frac{1}{4} \sum_{i=1}^{4} \text{score}_i
\]

Each sub-score includes:
\begin{itemize}
  \item \textbf{Factuality}: Presence of numerical or time-bound facts
  \item \textbf{Authority}: Institutional tone or cited sources
  \item \textbf{Utility}: Actionable insights or definitions
  \item \textbf{Clarity}: Syntactic fluency and paragraph modularity
\end{itemize}

\textbf{Threshold:} $\geq 0.70$

\subsection{Structural Readiness Score}

\textbf{Definition:} A schema-based binary score evaluating the extent to which a page uses structured markup conforming to Schema.org types relevant for generative understanding.

\[
\text{Structural\_Readiness} = \frac{\text{Valid schema types used}}{\text{Expected schema types}}
\]

\textbf{Schema types considered:} \texttt{Article}, \texttt{FAQPage}, \texttt{Person}, \texttt{WebPage}

\textbf{Threshold:} $\geq 0.80$

\subsection{Modular Extractability Score}

\textbf{Definition:} The proportion of a document that is easily separable into extractable, stand-alone units such as questions, steps, bullet points, or key takeaways.

\[
\text{Modular\_Extractability} = \frac{\text{\# modular units}}{\text{\# total blocks (paragraphs + lists)}}
\]

\textbf{Modular units include:}
\begin{itemize}
  \item Stand-alone 3--5 sentence paragraphs
  \item Numbered or bulleted lists
  \item FAQ blocks
  \item Inline definitions or quote blocks
\end{itemize}

\textbf{Threshold:} $\geq 0.65$

\subsection{Multi-Modal Adaptability Score}

\textbf{Definition:} Measures whether the content contains elements that enable multimodal recomposition by LLMs—such as figure descriptions, audio summary outlines, or tabular data.

\[
\text{Multi\_Modal\_Adaptability} = \frac{\text{Alternate format sections}}{\text{Total sections}}
\]

\textbf{Detected features include:}
\begin{itemize}
  \item \texttt{<figure>} or \texttt{<table>} elements with captions
  \item Podcast/video outlines (bulleted)
  \item Data tables with headers
\end{itemize}

\textbf{Threshold:} $\geq 0.60$

These metrics align with and expand on the visibility functions and impression metrics introduced in GEO-Bench \cite{aggarwal2024geo}. They support reproducible benchmarking across domains and platforms.
