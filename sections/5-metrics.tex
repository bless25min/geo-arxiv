\section{Evaluation Metrics} \label{sec:metrics}

To systematically assess the effectiveness of GEO interventions, we define five complementary evaluation metrics that measure different dimensions of content visibility within generative engines. These indicators are grounded in existing citation theories \cite{aggarwal2024geo, liu2023verifiability, menick2022quotes} and adapted for real-world observability and automation.

\subsection{AIO Semantic Focus Score}

\textbf{Definition:} The share of a document’s sentences that exhibit high semantic alignment with a declared topic or entity of focus.

Let $S$ be the total number of sentences and $S_t \subset S$ be those that contain or reinforce the primary topic entity $T$, as annotated via Named Entity Recognition (NER) or manual labeling.

\[
\text{AIO\_Semantic\_Focus}(T) = \frac{|S_t|}{|S|}
\]

\textbf{Threshold:} $\geq 0.75$

\textbf{Usage:} Diagnoses clarity of anchoring (Layer 1). A low score implies excessive topic drift.

\subsection{Citation Potential Score}

\textbf{Definition:} A composite metric scoring a paragraph or document on four citation-enhancing dimensions identified in GEO-Bench: factual density, authoritativeness, utility, and presentation clarity.

We adapt a simplified linear aggregation based on normalized human rater scores or GPT-assisted rubric scoring (following G-Eval templates).

\[
\text{Citation\_Potential} = \frac{1}{4} \sum_{i=1}^{4} \text{score}_i
\]

Each sub-score includes:
\begin{itemize}
  \item \textbf{Factuality}: Presence of numerical or time-bound facts
  \item \textbf{Authority}: Institutional tone or cited sources
  \item \textbf{Utility}: Actionable insights or definitions
  \item \textbf{Clarity}: Syntactic fluency and paragraph modularity
\end{itemize}

\textbf{Threshold:} $\geq 0.70$

\subsection{Structural Readiness Score}

\textbf{Definition:} A schema-based binary score evaluating the extent to which a page uses structured markup conforming to Schema.org types relevant for generative understanding.

\[
\text{Structural\_Readiness} = \frac{\text{Valid schema types used}}{\text{Expected schema types}}
\]

\textbf{Schema types considered:} \texttt{Article}, \texttt{FAQPage}, \texttt{Person}, \texttt{WebPage}

\textbf{Threshold:} $\geq 0.80$

\subsection{Modular Extractability Score}

\textbf{Definition:} The proportion of a document that is easily separable into extractable, stand-alone units such as questions, steps, bullet points, or key takeaways.

\[
\text{Modular\_Extractability} = \frac{\text{\# modular units}}{\text{\# total blocks (paragraphs + lists)}}
\]

\textbf{Modular units include:}
\begin{itemize}
  \item Stand-alone 3--5 sentence paragraphs
  \item Numbered or bulleted lists
  \item FAQ blocks
  \item Inline definitions or quote blocks
\end{itemize}

\textbf{Threshold:} $\geq 0.65$

\subsection{Multi-Modal Adaptability Score}

\textbf{Definition:} Measures whether the content contains elements that enable multimodal recomposition by LLMs—such as figure descriptions, audio summary outlines, or tabular data.

\[
\text{Multi\_Modal\_Adaptability} = \frac{\text{Alternate format sections}}{\text{Total sections}}
\]

\textbf{Detected features include:}
\begin{itemize}
  \item \texttt{<figure>} or \texttt{<table>} elements with captions
  \item Podcast/video outlines (bulleted)
  \item Data tables with headers
\end{itemize}

\textbf{Threshold:} $\geq 0.60$

These metrics align with and expand on the visibility functions and impression metrics introduced in GEO-Bench \cite{aggarwal2024geo}. They support reproducible benchmarking across domains and platforms.

\subsection{Layer-Based Evaluation Protocols}

To validate the operational effectiveness of each semantic layer within the GEO framework, we adopted a set of comparative and correlation-based metrics:

\begin{itemize}
  \item \textbf{Top-$k$ Overlap}: For each query variant, we computed the proportion of citation candidates shared with the original query’s Top-5 results. High overlap indicates stability in citation targets despite surface linguistic variation—especially relevant for Layer 2 testing.
  
  \item \textbf{Kendall’s Tau ($\tau$)}: To assess ranking consistency, we calculated the rank-order correlation of citations retrieved across different query formulations. Values close to $1$ reflect strong alignment in perceived relevance despite alternate phrasing.
  
  \item \textbf{Layer Hit Rate}: We annotated the retrieved citations to determine whether the triggered content stemmed from additions or rewrites introduced specifically for Layer 2 context triggering. A higher proportion of such hits suggests active contribution from semantic breadth.
  
  \item \textbf{Semantic Embedding Distance}: Using Sentence-BERT cosine distance, we measured linguistic drift between original and variant queries. This provided an orthogonal indicator of how different phrasings still led to shared citations, supporting the robustness of contextual retrieval.
\end{itemize}

These metrics collectively enabled both quantitative comparison across layers and longitudinal tracking of citation resilience under paraphrase, classification shifts, and stylistic variation.

